{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Improving the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will improve the search index and query functions from the previous assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Defining Auxiliary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is copied from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper(title='Social behaviors and gray matter volumes of brain areas supporting social cognition in children and adolescents with prenatal alcohol exposure.', authors=['de Water E', 'Rockhold MN', 'Roediger DJ', 'Krueger AM', 'Mueller BA', 'Boys CJ', 'Schumacher MJ', 'Mattson SN', 'Jones KL', 'Lim KO', 'Wozniak JR'], year=2021, doi='10.1016/j.brainres.2021.147388')\n"
     ]
    }
   ],
   "source": [
    "import pickle, bz2, re\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "from IPython.display import display, HTML\n",
    "from math import log10, sqrt\n",
    "\n",
    "Summaries_file = 'data/cognition_Summaries.pkl.bz2'\n",
    "Abstracts_file = 'data/cognition_Abstracts.pkl.bz2'\n",
    "\n",
    "Summaries = pickle.load( bz2.BZ2File( Summaries_file, 'rb' ) )\n",
    "Abstracts = pickle.load( bz2.BZ2File( Abstracts_file, 'rb' ) )\n",
    "\n",
    "paper = namedtuple( 'paper', ['title', 'authors', 'year', 'doi'] )\n",
    "for (id, paper_info) in Summaries.items():\n",
    "    Summaries[id] = paper( *paper_info )\n",
    "\n",
    "print(Summaries[33621483])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function that tokenizes a string in a rather naive way. Can be extended later.\n",
    "    \"\"\"\n",
    "    return text.split(' ')\n",
    "\n",
    "def preprocess(tokens):\n",
    "    \"\"\"\n",
    "    Perform linguistic preprocessing on a list of tokens. Can be extended later.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        result.append(token.lower())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary( id, show_abstract=False, show_id=True, extra_text='' ):\n",
    "    \"\"\"\n",
    "    Function for printing a paper's summary through IPython's Rich Display System.\n",
    "    Trims long author lists, and adds a link to the paper's DOI (when available).\n",
    "    \"\"\"\n",
    "    s = Summaries[id]\n",
    "    lines = []\n",
    "    title = s.title\n",
    "    if s.doi != '':\n",
    "        title = '<a href=http://dx.doi.org/{:s}>{:s}</a>'.format(s.doi, title)\n",
    "    title = '<strong>' + title + '</strong>'\n",
    "    lines.append(title)\n",
    "    authors = ', '.join( s.authors[:20] ) + ('' if len(s.authors) <= 20 else ', ...')\n",
    "    lines.append(str(s.year) + '. ' + authors)\n",
    "    if (show_abstract):\n",
    "        lines.append('<small><strong>Abstract:</strong> <em>{:s}</em></small>'.format(Abstracts[id]))\n",
    "    if (show_id):\n",
    "        lines.append('[ID: {:d}]'.format(id))\n",
    "    if (extra_text != ''):\n",
    "         lines.append(extra_text)\n",
    "    display( HTML('<br>'.join(lines)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = defaultdict(list)\n",
    "\n",
    "for id in sorted(Summaries.keys()):\n",
    "    term_set = set(preprocess(tokenize(Summaries[id].title)))\n",
    "    if id in Abstracts:\n",
    "        term_set.update(preprocess(tokenize(Abstracts[id])))\n",
    "    for term in term_set:\n",
    "        inverted_index[term].append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see from the results of the last assignment, our simple index doesn't handle punctuation and the difference between singular and plural versions of the same word very well. We won't go much into the details of tokenization and linguistic analysis here, because we also want to focus on scoring and ranking below. Therefore, we are using an existing library for tokenizatoin and stemming, namely the NLTK package. The following line will install NLTK if necessary (or you have to follow [these instructions](http://www.nltk.org/install.html) if that doesn't work):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/riles/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: click in /Users/riles/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/riles/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/riles/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in /Users/riles/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install --user nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "  Good muffins cost $3.88\n",
      "in New York.  Please buy me two of them.\n",
      "\n",
      "Thanks.\n",
      "TOKENIZE:  ['Good', 'muffins', 'cost', '$3.88\\nin', 'New', 'York.', '', 'Please', 'buy', 'me', 'two', 'of', 'them.\\n\\nThanks.']\n",
      "WORD TOKENIZE:  ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/riles/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.'''\n",
    "\n",
    "print('INPUT TEXT:\\n ', s)\n",
    "\n",
    "print('TOKENIZE: ', tokenize(s))\n",
    "print('WORD TOKENIZE: ', word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process\n",
      "appl\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"processes\"))\n",
    "print(stemmer.stem(\"apples\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important method to improve our search results is to rank them, which can be done by calculating a score for each document based on the matching terms from the query. One such scoring method is *tf-idf*, which comes with several variants, as explained in the lecture slides.\n",
    "\n",
    "In order to quickly calculate the scores for a term/document combination, we'll need quick access to a couple of things:\n",
    "\n",
    "- tf(t,d): How often does a term occur in a document\n",
    "- df(t): In how many documents does a term occur\n",
    "- num_documents: The number of documents in our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = defaultdict(Counter)\n",
    "\n",
    "for doc_id in Summaries.keys():\n",
    "    tokens = preprocess(tokenize(Summaries[doc_id].title))\n",
    "    if (doc_id in Abstracts):\n",
    "        tokens.extend(preprocess(tokenize(Abstracts[doc_id])))\n",
    "    tf_matrix[doc_id] = Counter(tokens)\n",
    "\n",
    "def tf(t,d):\n",
    "    return float(tf_matrix[d][t])\n",
    "\n",
    "def df(t):\n",
    "    return float(len(inverted_index[t]))\n",
    "\n",
    "num_documents = float(len(Summaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test these functions with some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "35.0\n",
      "102110.0\n"
     ]
    }
   ],
   "source": [
    "print(tf('amsterdam', 20594617))\n",
    "print(df('amsterdam'))\n",
    "print(num_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these helper functions, we can now easily calculate the _tf-idf_ weights of a term in a document by implementing the weighting formula from the slides, which you will do in the assignments below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name: Riley Crahen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Implement in the code block below the `smarter_tokenize_and_preprocess` function using NLTK's functions for tokenization and stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['where', 'are', 'my', 'appl', '!']\n"
     ]
    }
   ],
   "source": [
    "# Your code here:\n",
    "def smarter_tokenize_and_preprocess(text):\n",
    "    split_tokens = word_tokenize(text);\n",
    "    \n",
    "    result = []\n",
    "    for token in split_tokens:\n",
    "        result.append(stemmer.stem(token.lower()))\n",
    "    return result\n",
    "    \n",
    "\n",
    "print(smarter_tokenize_and_preprocess(\"where are My apples!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a smarter index based on this function. For practical purposes, the code below generates the smarter index on a subset of the data, as generating an index with stemming on the entire set would take too much time. (You don't need to change or add anything in the code block below. Just leave it as it is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, we create our smarter index (based on a subset of the documents for demonstration purposes)\n",
    "smarter_index = defaultdict(list)\n",
    "\n",
    "# Here we define the subset (somewhat arbitrary):\n",
    "subset_of_ids = list(key for key in Summaries.keys() if 33000000 <= key < 34000000)\n",
    "\n",
    "# Building our smarter index:\n",
    "for id in sorted(subset_of_ids):\n",
    "    term_set = set(smarter_tokenize_and_preprocess(Summaries[id].title))\n",
    "    if id in Abstracts:\n",
    "        term_set.update(smarter_tokenize_and_preprocess(Abstracts[id]))\n",
    "    for term in term_set:\n",
    "        smarter_index[term].append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the `smarter_and_query` function, based on the function `smarter_tokenize_and_preprocess` you defined above and accessing our new index `smarter_index`. You can start from copying the code for `and_query` from the last assignment. For that to work, you'll also have to copy the code for the `and_merge` function from the last assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smarter and_query based on the smarter tokenize and preprocess functions\n",
    "\n",
    "def and_merge(sorted_list1, sorted_list2):  \n",
    "    merged_list = []  \n",
    "    # first we make copies of the lists, so we don't modify the existing lists in the index:\n",
    "    list1 = list(sorted_list1)  \n",
    "    list2 = list(sorted_list2)  \n",
    "    while (list1 and list2):  \n",
    "        if (list1[0] < list2[0]):  \n",
    "            list1.pop(0)  \n",
    "        elif (list1[0] > list2[0]):  \n",
    "            list2.pop(0)  \n",
    "        else:  \n",
    "            merged_list.append(list1[0])  \n",
    "            list1.pop(0)  \n",
    "            list2.pop(0)  \n",
    "    return merged_list  \n",
    "\n",
    "def smarter_and_query(query_string):  \n",
    "    query_words = smarter_tokenize_and_preprocess(query_string)\n",
    "    first_word = query_words[0]  \n",
    "    remaining_words = query_words[1:]  \n",
    "    and_list = smarter_index[first_word]  \n",
    "    for t in remaining_words:  \n",
    "        and_list = and_merge(and_list, smarter_index[t])  \n",
    "    return and_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Run the query \"quality controls checklist\" with the new `smarter_and_query` function from task 1. Does it return paper *33621483*? Explain what our new smarter function specifically contributes to the result (as compared to our previous naive implementations for tokenization and preprocessing)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qualiti', 'control', 'checklist']\n",
      "[33066649, 33621483]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1016/j.brainres.2021.147388>Social behaviors and gray matter volumes of brain areas supporting social cognition in children and adolescents with prenatal alcohol exposure.</a></strong><br>2021. de Water E, Rockhold MN, Roediger DJ, Krueger AM, Mueller BA, Boys CJ, Schumacher MJ, Mattson SN, Jones KL, Lim KO, Wozniak JR<br><small><strong>Abstract:</strong> <em>The goal of this study was to examine: 1) differences in parent-reported prosocial and antisocial behaviors between children and adolescents with and without prenatal alcohol exposure (PAE); 2) differences in gray matter volumes of brain areas supporting social cognition between children and adolescents with and without PAE; 3) correlations between gray matter volumes of brain areas supporting social cognition and parent-reported prosocial and antisocial behaviors. Parents of children and adolescents ages 8-16 years completed measures on their prosocial and antisocial behaviors (i.e., Behavior Assessment Scale for Children, Vineland Adaptive Behaviors Scales, and Child Behavior Checklist) (n = 84; 41 with PAE, 43 without PAE). Seventy-nine participants (40 with PAE, 39 without PAE) also completed a structural Magnetic Resonance Imaging (MRI) scan with quality data. Gray matter volumes of seven brain areas supporting social cognitive processes were computed using automated procedures (FreeSurfer 6.0): bilateral fusiform gyrus, superior temporal gyrus, medial orbitofrontal cortex, lateral orbitofrontal cortex, posterior cingulate cortex, precuneus, and temporal pole. Children and adolescents with PAE showed decreased prosocial behaviors and increased antisocial behaviors as well as smaller volumes of the precuneus and lateral orbitofrontal cortex, even when controlling for total intracranial volume. Social brain volumes were not significantly correlated with prosocial or antisocial behaviors. These findings suggest that children and adolescents with PAE show worse social functioning and smaller volumes of brain areas supporting self-awareness, perspective-taking and emotion-regulation than their same-age peers without PAE.</em></small><br>[ID: 33621483]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#smarter_and_query(\"quality controls checklist\")\n",
    "print(smarter_tokenize_and_preprocess(\"quality controls checklist\"))\n",
    "print(smarter_and_query(\"quality controls checklist\"))\n",
    "display_summary(33621483, show_abstract=True, show_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "Paper with ID 33621483 contains the words: \"quality\", \"controlling\", and \"Checklist', which get stemmed to: 'qualiti', 'control', and 'checklist' respectively. This is the same as what the query \"quality control checklist\" gets tokenized to. Therefore it will show up in the smarter_and_query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Now we move to a different subject and use our old index again. That is, we **don't** use the smarter functions defined above for tasks 3 to 5!\n",
    "\n",
    "Create a function `tfidf(t,d)` that returns the tf-idf score of term `t` in document `d` by using `tf(t,d)`, `df(t)` and `num_documents` as defined above. To do this, first implement a function `idf(t)` to calculate the inverse document frequency, and then use this function to calculate the full tf-idf. Use the _add-one-smoothing_ version of idf, so we don't run into problems with terms that don't appear in the collection at all. The relevant formulas can be found on the lecture slides. Use tf-idf with plain (non-logarithmic) term frequency, as applied by scoring variant `ntn`. Test your function with the examples shown below. You can use the `log10(n)` function to calculate the base 10 logarithm.\n",
    "\n",
    "Again, use our old (non-smart) index for this task and the tasks below, and **not** the functions defined in tasks 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4527700286063565\n"
     ]
    }
   ],
   "source": [
    "# Your code here:\n",
    "def tfidf(t,d):\n",
    "    return tf(t, d)*idf(t);\n",
    "\n",
    "def idf(t):\n",
    "    return log10((num_documents+1)/(df(t)+1));\n",
    "\n",
    "print(tfidf(\"amsterdam\", 20594617))\n",
    "\n",
    "\n",
    "#print(df(20594617));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Create a function `query_ntn_nnn(query_string)`, which accepts as input a single query string of one or more words, and returns or prints a list of (up to) 10 best matching documents, along with their score. Use _tf-idf_ to calculate document scores based on the query, applying variant `ntn.nnn`, as above (see the formula for the `ntn.nnn` version of scoring on the lecture slides). Use an auxiliary function `score_ntn_nnn` to calculate the score. The results should be shown in descending order by score.\n",
    "\n",
    "You can start by copying your functions `or_merge` and `or_query` from assignment 2, then expand that to rank the results, making use of the `tfidf(t,d)` function you created above.\n",
    "\n",
    "Demonstrate your function by giving it the exemplary query string \"happiness and sadness of young adults\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "def query_ntn_nnn(query_string):\n",
    "    #create default dict with key = word in query, value = ntn.nnn score\n",
    "    document_scores = defaultdict(int);\n",
    "    \n",
    "    #for each document:\n",
    "    for doc_id in or_query(query_string):\n",
    "        #calculate total score\n",
    "        document_scores[doc_id] = score_ntn_nnn(query_string, doc_id);\n",
    "\n",
    "    return sorted(document_scores.items(), key=lambda i:i[1], reverse=True)[:10]\n",
    "\n",
    "def score_ntn_nnn(q, d):\n",
    "    cur_score = 0;\n",
    "    for term in preprocess(tokenize(q)):\n",
    "        cur_score+=tfidf(term, d);\n",
    "    return cur_score;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def or_query(inputString):\n",
    "    tokenizedInput = preprocess(tokenize(inputString));\n",
    "    \n",
    "    matchingDocuments = inverted_index[tokenizedInput[0]];\n",
    "    \n",
    "    for x in range(len(tokenizedInput)-1):\n",
    "        matchingDocuments = or_merge(matchingDocuments, inverted_index[tokenizedInput[x+1]]);\n",
    "    \n",
    "    return matchingDocuments;\n",
    "\n",
    "def or_merge(l1, l2):\n",
    "    combined = [];\n",
    "    index1 = 0;\n",
    "    index2 = 0;\n",
    "\n",
    "    while(index1 < len(l1) and index2 < len(l2)):\n",
    "        if(l1[index1] < l2[index2]):\n",
    "            combined.append(l1[index1]);\n",
    "            index1+=1;\n",
    "        elif(l2[index2] < l1[index1]):\n",
    "            combined.append(l2[index2])\n",
    "            index2+=1;\n",
    "        else:\n",
    "            combined.append(l1[index1])\n",
    "            #increment both\n",
    "            index1+=1;\n",
    "            index2+=1;\n",
    "\n",
    "        #add all remaining values\n",
    "    while(index1 < len(l1)):\n",
    "        combined.append(l1[index1]);\n",
    "        index1+=1;\n",
    "    while(index2 < len(l2)):\n",
    "        combined.append(l2[index2]);\n",
    "        index2+=1;\n",
    "\n",
    "    return combined;    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24046755, 21.74695831423794), (24628461, 20.713595738895215), (31817633, 20.425747317841044), (35095690, 19.866026923007695), (19389462, 19.85220333774405), (34899267, 17.92865427406855), (32038362, 16.6102165732608), (24970235, 16.348395873280644), (35782096, 15.586692232912489), (31865171, 15.47475995098112)]\n"
     ]
    }
   ],
   "source": [
    "print(query_ntn_nnn(\"happiness and sadness of young adults\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fpsyg.2013.00592>Reverse correlating trustworthy faces in young and older adults.</a></strong><br>2013. Ethier-Majcher C, Joubert S, Gosselin F<br><small><strong>Abstract:</strong> <em>Little is known about how older persons determine if someone deserves their trust or not based on their facial appearance, a process referred to as \"facial trustworthiness.\"In the past few years, Todorov and colleagues have argued that, in young adults, trustworthiness judgments are an extension of emotional judgments, and therefore, that trust judgments are made based on a continuum between anger and happiness (Todorov, 2008; Engell et al., 2010). Evidence from the literature on emotion processing suggest that older adults tend to be less efficient than younger adults in the recognition of negative facial expressions (Calder et al., 2003; Firestone et al., 2007; Ruffman et al., 2008; Chaby and Narme, 2009). Based on Todorov';s theory and the fact that older adults seem to be less efficient than younger adults in identifying emotional expressions, one could expect that older individuals would have different representations of trustworthy faces and that they would use different cues than younger adults in order to make such judgments. We verified this hypothesis using a variation of Mangini and Biederman's (2004) reverse correlation method in order to test and compare classification images resulting from trustworthiness (in the context of money investment), from happiness, and from anger judgments in two groups of participants: young adults and older healthy adults. Our results show that for elderly participants, both happy and angry representations are correlated with trustworthiness judgments. However, in young adults, trustworthiness judgments are mainly correlated with happiness representations. These results suggest that young and older adults differ in their way of judging trustworthiness. </em></small><br>[ID: 24046755]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(24046755, show_abstract=True, show_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "In this last task, you should create a second version of the query function from Task 4, called `query_ntc_ntc`. This second version should use, as its name suggests, variant `ntc.ntc` instead of `ntn.nnn`, and therefore apply the cosine similarity measure, in addition to applying _tf-idf_. For this, consult the formula for variant `nnc.nnc` on the lecture slides and adopt it to include the _idf_ metric (that is, add the `t` element of `ntc`). (You can drop the square root of |q| in the formula, as indicated on the slides.)\n",
    "\n",
    "As a first step, we can calculate beforehand the length of all document vectors (because they don't depend on the query) for document vectors consisting of _tf-idf_ values. The code below does just that, assuming that you defined the function `tfidf(t,d)` above (don't change this code block, just run it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_length_values = defaultdict(int)\n",
    "\n",
    "for doc_id in Summaries.keys():\n",
    "    l = 0\n",
    "    for t in tf_matrix[doc_id].keys():\n",
    "        l += tfidf(t,doc_id) ** 2\n",
    "    tfidf_length_values[doc_id] = sqrt(l)\n",
    "\n",
    "def tfidf_length(d):\n",
    "    return tfidf_length_values[d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the length of a document vector by calling `tfidf_length(d)`.\n",
    "\n",
    "Based on this, you can now implement `query_ntc_ntc` in the code block below. You should again first define an auxiliary function, called `score_ntc_ntc`. You can start by copy-pasting the code from Task 4.\n",
    "\n",
    "To output the results, use the provided `display_summary` function to make the output a bit more like the results page of a search engine. Lastly, demonstrate your `query_ntc_ntc` function with the same example query as above: \"happiness and sadness of young adults\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1177/0956797619895282>Young Children Use Probability to Infer Happiness and the Quality of Outcomes.</a></strong><br>2020. Doan T, Friedman O, Denison S<br><small><strong>Abstract:</strong> <em>Happiness with an outcome often depends on whether better or worse outcomes were initially more likely. In five experiments, we found that young children (</em></small><br>[ID: 31868569]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.7717/peerj.13673>Happiness, depression, physical activity and cognition among the middle and old-aged population in China: a conditional process analysis.</a></strong><br>2022. Shi X, He X, Pan D, Qiao H, Li J<br><small><strong>Abstract:</strong> <em>Happiness is one variable of subjective well-being, which has been increasingly shown to have protective effects on health. Although the association between happiness and cognition has been established, the mechanism by which happiness leads to cognition remains unclear. Since happiness, depression, and physical activity may all be related to cognition, and happiness is related to depression and physical activity, this study explored the effect of depression and physical activity on the relationship between happiness and cognition among middle and old-aged individuals in China.</em></small><br>[ID: 35782096]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1111/jir.12868>Language and executive functioning in young adults with Down syndrome.</a></strong><br>2022. Kristensen K, Lorenz KM, Zhou X, Piro-Gambetti B, Hartley SL, Godar SP, Diel S, Neubauer E, Litovsky RY<br><small><strong>Abstract:</strong> <em>This study examined the association between executive functioning and language in young adults with Down syndrome (DS).</em></small><br>[ID: 34288180]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.3389/fpsyg.2013.00592>Reverse correlating trustworthy faces in young and older adults.</a></strong><br>2013. Ethier-Majcher C, Joubert S, Gosselin F<br><small><strong>Abstract:</strong> <em>Little is known about how older persons determine if someone deserves their trust or not based on their facial appearance, a process referred to as \"facial trustworthiness.\"In the past few years, Todorov and colleagues have argued that, in young adults, trustworthiness judgments are an extension of emotional judgments, and therefore, that trust judgments are made based on a continuum between anger and happiness (Todorov, 2008; Engell et al., 2010). Evidence from the literature on emotion processing suggest that older adults tend to be less efficient than younger adults in the recognition of negative facial expressions (Calder et al., 2003; Firestone et al., 2007; Ruffman et al., 2008; Chaby and Narme, 2009). Based on Todorov';s theory and the fact that older adults seem to be less efficient than younger adults in identifying emotional expressions, one could expect that older individuals would have different representations of trustworthy faces and that they would use different cues than younger adults in order to make such judgments. We verified this hypothesis using a variation of Mangini and Biederman's (2004) reverse correlation method in order to test and compare classification images resulting from trustworthiness (in the context of money investment), from happiness, and from anger judgments in two groups of participants: young adults and older healthy adults. Our results show that for elderly participants, both happy and angry representations are correlated with trustworthiness judgments. However, in young adults, trustworthiness judgments are mainly correlated with happiness representations. These results suggest that young and older adults differ in their way of judging trustworthiness. </em></small><br>[ID: 24046755]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1177/0091415016668355>Age Differences in Information Use While Making Decisions: Resource Limitations or Processing Differences?</a></strong><br>2016. Jacobs-Lawson JM, Schumacher MM, Wackerbarth SB<br><small><strong>Abstract:</strong> <em>Recent research on the decision-making abilities of older adults has shown that they use less information than young adults. One explanation ascribes this age difference to reductions in cognitive abilities with age. The article includes three experimental studies that focused on determining the conditions in which older and young adults would display dissimilar information processing characteristics. Findings from Studies 1 and 2 demonstrated that older adults are not necessarily at greater disadvantage than young adults in decision contexts that demand more information processing resources. Findings from Study 3 indicated that older adults when faced with decisions that require greater processing are likely to use a strategy that reduces the amount of information needed, whereas younger adults rely on strategies that utilize more resources. Combined the findings indicate that older adults change their decision-making strategies based on the context and information provided. Furthermore, support is provided for processing difference.</em></small><br>[ID: 27655952]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1111/jir.12163>The association between a single bout of moderate physical activity and executive function in young adults with Down syndrome: a preliminary study.</a></strong><br>2015. Chen CC, Ringenbach SD, Crews D, Kulinna PH, Amazeen EL<br><small><strong>Abstract:</strong> <em>This study was aimed at investigating the impact of a single exercise intervention on executive function in young adults with Down syndrome (DS).</em></small><br>[ID: 25171600]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.2174/1874609814666210716111022>Young and Middle-aged Adults Differ in Neural Correlate of Sustained Attention: A fNIRS Study.</a></strong><br>2021. Shenoy S, Khandekar P, Sathe A<br><small><strong>Abstract:</strong> <em>The aim of the study was to assess the activation of prefrontal cortex during sustained attention task in young and middle aged adults using Functional near infrared spectroscopy system.</em></small><br>[ID: 34279210]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Using a Participatory Action Research Design to Develop an Application Together with Young Adults with Spina Bifida.</strong><br>2015. Lidström H, Lindskog-Wallander M, Arnemo E<br><small><strong>Abstract:</strong> <em>Young adults with spina bifida often have cognitive difficulties. As a result, young adults with disabilities are facing challenges with respect to housing, education, relationships and vocation which increases risk of unemployment.</em></small><br>[ID: 26294472]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1037//0882-7974.15.1.157>Converging evidence that visuospatial cognition is more age-sensitive than verbal cognition.</a></strong><br>2000. Jenkins L, Myerson J, Joerding JA, Hale S<br><small><strong>Abstract:</strong> <em>In 3 separate experiments, the same samples of young and older adults were tested on verbal and visuospatial processing speed tasks, verbal and visuospatial working memory tasks, and verbal and visuospatial paired-associates learning tasks. In Experiment 1, older adults were generally slower than young adults on all speeded tasks, but age-related slowing was much more pronounced on visuospatial tasks than on verbal tasks. In Experiment 2, older adults showed smaller memory spans than young adults in general, but memory for locations showed a greater age difference than memory for letters. In Experiment 3, older adults had greater difficulty learning novel information than young adults overall, but older adults showed greater deficits learning visuospatial than verbal information. Taken together, the differential deficits observed on both speeded and unspeeded tasks strongly suggest that visuospatial cognition is generally more affected by aging than verbal cognition.</em></small><br>[ID: 10755297]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1111/dmcn.15225>Walking speed and patient-reported outcomes in young adults with cerebral palsy.</a></strong><br>2022. MacCarthy M, Heyn P, Tagawa A, Carollo J<br><small><strong>Abstract:</strong> <em>To examine the relationship between quantitative gait measurements and self-reported physical, psychological, cognitive, and social function status in young adults with cerebral palsy (CP).</em></small><br>[ID: 35366333]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here:\n",
    "def query_ntc_ntc(query_string):\n",
    "    #create default dict with key = word in query, value = ntn.nnn score\n",
    "    document_scores = defaultdict(int);\n",
    "    \n",
    "    #for each document:\n",
    "    for doc_id in or_query(query_string):\n",
    "        #calculate total score\n",
    "        document_scores[doc_id] = score_ntc_ntc(query_string, doc_id);\n",
    "\n",
    "    top_10_list = sorted(document_scores.items(), key=lambda i:i[1], reverse=True)[:10]\n",
    "\n",
    "    for pair in top_10_list:\n",
    "        display_summary(pair[0], show_abstract=True, show_id=True)\n",
    "\n",
    "def score_ntc_ntc(q,d):\n",
    "    cur_score = 0;\n",
    "    for term in preprocess(tokenize(q)):\n",
    "        cur_score+=(tfidf(term, d)/tfidf_length(d));\n",
    "    return cur_score;\n",
    "    \n",
    "query_ntc_ntc(\"happiness and sadness of young adults\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the answers to the assignment via Canvas as a modified version of this Notebook file (file with `.ipynb` extension) that includes your code and your answers.\n",
    "\n",
    "Before submitting, restart the kernel and re-run the complete code (**Kernel > Restart & Run All**), and then check whether your assignment code still works as expected.\n",
    "\n",
    "Don't forget to add your name, and remember that the assignments have to be done **individually**, and that code sharing or copying are **strictly forbidden** and will be punished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
